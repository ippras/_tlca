## 1. Метрики, основанные на Lp-нормах (геометрические расстояния, обобщение Евклидова и Манхэттенского)

[47 regression metrics, 20 classification metrics, 44 clustering metrics](https://permetrics.readthedocs.io/en/v1.4.2/pages/clustering.html)

Similarity between two data points
* Euclidean distance
* Manhattan distance
* Minkowski distance
* Chebyshev distance

### 1.1. Евклидово расстояние (Euclidean Distance)

* **Диапазон:** `[0, +∞)`
* **Что показывает:** Прямое геометрическое расстояние между двумя точками в многомерном пространстве. Чем меньше значение, тем ближе векторы.
* **Когда полезно:** Когда абсолютные разности по всем компонентам важны и имеют сопоставимую шкалу. Часто используется в задачах кластеризации и поиска ближайших соседей.

### 1.2. Манхэттенское расстояние (Manhattan Distance / City Block Distance)

* **Диапазон:** `[0, +∞)`
* **Что показывает:** Сумма абсолютных разностей по каждой компоненте. Расстояние между двумя точками в многомерном пространстве, которое нужно пройти, двигаясь только вдоль осей.
* **Когда полезно:** Когда перемещения по осям не могут происходить одновременно, или когда выбросы не должны сильно влиять на общее расстояние (по сравнению с Евклидовым, где квадраты увеличивают влияние больших разностей).

### 1.3. Расстояние Чебышёва (Chebyshev Distance / L-infinity norm / Maximum value distance)

* **Диапазон:** `[0, +∞)`. Если компоненты являются долями/процентами в диапазоне [0,1], то максимальное расстояние будет `[0,1]`.
* **Что показывает:** Максимальное абсолютное различие по любой из компонент.
* **Когда полезно:** Если важно знать "наихудшее" расхождение по одной конкретной компоненте. Используется в логистике, теории игр (например, минимальное число ходов короля на шахматной доске).

### 1.4. Расстояние Минковского (Minkowski Distance)

* **Диапазон:** `[0, +∞)`
* **Что показывает:** Обобщенная метрика расстояния. При `p=1` это Манхэттенское расстояние, при `p=2` - Евклидово, при `p -> ∞` - Чебышёва. В скрипте используется `p=3`.
* **Когда полезно:** Позволяет варьировать чувствительность к большим разностям через параметр `p`. Большие значения `p` придают больший вес большим разностям между компонентами.

## 2. Метрики, основанные на корреляции (оценивают сходство формы/тренда)

### 2.1. Расстояние Пирсона (Pearson Distance)

* **Диапазон:** `[0, 2]`
* **Что показывает:** Насколько векторы линейно зависимы. `0` означает идеальную положительную линейную корреляцию, `1` - отсутствие корреляции, `2` - идеальную отрицательную линейную корреляцию.
* **Когда полезно:** Когда важна тенденция (линейная) изменения значений, а не их абсолютные величины или масштаб. Например, для поиска схожих временных рядов по форме.

### 2.2. Расстояние Спирмена (Spearman Distance)

* **Диапазон:** `[0, 2]`
* **Что показывает:** Насколько векторы монотонно зависимы (основано на рангах значений, а не на самих значениях). `0` означает идеальную положительную монотонную связь, `1` - отсутствие монотонной связи, `2` - идеальную отрицательную монотонную связь.
* **Когда полезно:** Когда важна монотонная связь (не обязательно линейная), и данные могут содержать выбросы или не иметь линейной зависимости. Менее чувствительно к выбросам, чем расстояние Пирсона.

## 3. Метрики для сравнения вероятностных распределений (*используют нормализованные векторы P, Q (вероятностные распределения)*)

### 3.1. Дивергенция/Расстояние Дженсена-Шеннона (Jensen-Shannon Divergence/Distance)

* **Диапазон (расстояние):** `[0, sqrt(ln(2))]` (для натурального логарифма).
* **Что показывает:** Симметричная и сглаженная мера различия между двумя вероятностными распределениями. `0` означает идентичные распределения.
* **Когда полезно:** Когда нужно сравнить два вероятностных распределения, и важна симметрия (в отличие от KL-дивергенции). Всегда имеет конечное значение, даже если в распределениях есть нули.

### 3.2. Дивергенция Кульбака-Лейблера (Kullback-Leibler Divergence / Relative Entropy)

* **Диапазон:** `[0, +∞)`. Может быть `+∞`, если `Q_i = 0` при `P_i > 0`.
* **Что показывает:** "Потеря информации" при аппроксимации истинного распределения P распределением Q. Несимметрична (`KL(P||Q) != KL(Q||P)`). `0` означает идентичные распределения.
* **Когда полезно:** В теории информации, машинном обучении для измерения различия между истинным и модельным распределением. Важно учитывать направление сравнения (какое распределение аппроксимируется каким).

### 3.3. Расстояние Хеллингера (Hellinger Distance)

* **Диапазон:** `[0, 1]`
* **Что показывает:** Мера сходства между двумя вероятностными распределениями. `0` означает идентичные распределения, `1` - полное несходство.
* **Когда полезно:** Для сравнения вероятностных распределений, особенно когда важно избежать проблем с нулями (в отличие от KL-дивергенции, если не применять сглаживание). Является истинной метрикой (удовлетворяет неравенству треугольника).

### 3.4. Расстояние Вассерштейна (Wasserstein Distance / Earth Mover's Distance)

* **Диапазон:** `[0, +∞)`
* **Что показывает:** Стоимость преобразования одного распределения вероятностей в другое, учитывая "расстояние" между элементами (категориями) распределений.
* **Когда полезно:** Когда важно не только различие в вероятностях, но и "расстояние" между значениями (или категориями), которым эти вероятности приписаны. Полезно для сравнения гистограмм или упорядоченных категориальных распределений, где порядок бинов имеет значение.

## 4. Метрики, основанные на пересечении/объединении или нормализованных суммах (часто для количественных данных или долей)

### 4.1. Несходство Брея-Кёртиса (Bray-Curtis Dissimilarity)

* **Диапазон:** `[0, 1]`
* **Что показывает:** Мера несходства, учитывающая как абсолютные разности, так и общую сумму значений. Нормализованная версия Манхэттенского расстояния. `0` означает идентичность, `1` - полное несходство.
* **Когда полезно:** В экологии для сравнения видового состава, в химии для сравнения профилей. Чувствительно к изменениям в доминирующих компонентах. Эквивалентно расстоянию Сёренсена-Дайса для неотрицательных векторов.

### 4.2. Расстояние Ружички (количественный Жаккар) (Ruzicka Distance / Quantitative Jaccard Distance)

* **Диапазон:** `[0, 1]`
* **Что показывает:** Мера несходства, основанная на отношении суммы покомпонентных минимумов к сумме покомпонентных максимумов. Является количественным аналогом индекса Жаккара. `0` означает идентичность.
* **Когда полезно:** Для сравнения количественных данных, где важно соотношение "общего" (пересечения) и "объединенного" вклада каждой компоненты.

### 4.3. Расстояние Сёренсена-Дайса (количественное) (Sørensen-Dice Distance)

* **Диапазон:** `[0, 1]`
* **Что показывает:** Мера несходства, основанная на отношении удвоенной суммы покомпонентных минимумов (пересечения) к сумме всех значений в обоих векторах. `0` означает идентичность, `1` - полное несходство.
* **Когда полезно:** Аналогично расстоянию Брея-Кёртиса (для неотрицательных векторов они эквивалентны). Часто используется в экологии, анализе изображений (для оценки перекрытия сегментов), сравнении множеств.

## 5. Другие метрики для сравнения векторов признаков (не вошедшие в предыдущие категории)

### 5.1. Косинусное расстояние (Cosine Distance)

* **Диапазон:** `[0, 1]` для неотрицательных векторов; `[0, 2]` в общем случае.
* **Что показывает:** Несходство направлений двух векторов, независимо от их магнитуды (длины). `0` означает одинаковое направление, `1` - ортогональные (для неотрицательных векторов).
* **Когда полезно:** Когда важна ориентация или "форма" профиля данных, а не их абсолютные значения. Часто используется в анализе текстов (например, с TF-IDF), рекомендательных системах.

### 5.2. Расстояние Канберры (Canberra Distance)

* **Диапазон:** `[0, +∞)` (или `[0, N]` где N - число ненулевых компонент, если один вектор нулевой, а другой нет).
* **Что показывает:** Взвешенная версия Манхэттенского расстояния. Каждая разность нормализуется суммой абсолютных значений компонент. `0` означает идентичность.
* **Когда полезно:** Чувствительно к малым изменениям, когда оба значения компонент малы, и менее чувствительно к большим изменениям, когда оба значения велики. Часто используется для данных с выбросами или строго неотрицательных данных (например, в экологии).