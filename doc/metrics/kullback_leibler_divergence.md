# Дивергенция Кульбака-Лейблера (Kullback-Leibler Divergence / Relative Entropy)

* **Диапазон:** `[0, +∞)`. Может быть `+∞`, если `Q_i = 0` при `P_i > 0`.
* **Что показывает:** "Потеря информации" при аппроксимации истинного распределения P распределением Q. Несимметрична (`KL(P||Q) != KL(Q||P)`). `0` означает идентичные распределения.
* **Когда полезно:** В теории информации, машинном обучении для измерения различия между истинным и модельным распределением. Важно учитывать направление сравнения (какое распределение аппроксимируется каким).
